{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1 (Normal-Normal conjugacy)\n",
    "\n",
    "Let $y$ be a random variable that is drawn from a normal distribution with mean $\\mu$ and standard deviation $\\sigma$. \n",
    "\n",
    "1. Consider a dataset $Y=(y_1,y_2,...,y_n)$ containing $n$ realizations of $y$. Write down the likelihood of $Y$ as a function of $\\mu$ and  $\\sigma$\n",
    "\n",
    "2. Assume that $\\mu$ itself is Normally distributed around some mean $\\theta$ with standard deviation $\\tau$. Show that upon observing $Y$, the posterior belief about $\\mu$ remains normally distributed. Derive the posterior mean and standard deviation of $\\mu$. What do you notice about the standard deviation? Comment your finding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Solution</b></summary>\n",
    "\n",
    "1. The likelihood of $Y$ can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "    L\\left(\\mu, \\sigma | Y \\right) &= \\prod_{i = 1}^{n} f\\left(y_{i}|\\mu, \\sigma\\right) \\\\\n",
    "                                   &= \\prod_{i = 1}^{n} \\frac{1}{\\sqrt{2\\pi \\sigma^{2}}}\\text{exp}\\left(-\\frac{\\left(y_{i}-\\mu\\right)^{2}}{2\\sigma^{2}}\\right).\n",
    "\\end{align*}\n",
    "\n",
    "2. By the Bayes' rule, we find the posterior distribution of $\\mu$ given the data $Y$:\n",
    "\n",
    "\\begin{align*}\n",
    "     p\\left(\\mu|Y,\\theta,\\tau,\\sigma\\right) &\\propto p\\left(Y|\\mu,\\sigma\\right)p\\left(\\mu|\\theta,\\tau\\right)\\\\\n",
    "        & \\propto \\text{exp}\\left(-\\frac{1}{2}[\\frac{1}{\\sigma^{2}}\\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2} + \\frac{\\left(\\mu-\\theta\\right)^{2}}{\\tau^{2}}]\\right).\n",
    "\\end{align*}\n",
    "\n",
    "Let $\\overline{y} = \\frac{1}{n}\\sum_{i=1}^{n}y_{i}$, by the Law of Total Variance, we have \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\sum_{i=1}^{n}\\left(y_{i}-\\mu\\right)^{2} = \\sum_{i=1}^{n}\\left(y_{i} - \\overline{y}\\right)^{2} + n\\left(\\overline{y}-\\mu\\right)^{2}.\n",
    "\\end{equation*}\n",
    "\n",
    "Thus the posterior is simplified to:\n",
    "\n",
    "\\begin{equation*}\n",
    "    p\\left(\\mu|Y,\\theta,\\tau,\\sigma\\right) \\propto \\text{exp}\\left(-\\frac{1}{2}[\\frac{n\\left(\\overline{y}-\\mu\\right)^{2}}{\\sigma^{2}} + \\frac{\\left(\\mu-\\theta\\right)^{2}}{\\tau^2}]\\right).\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "The exponent can be written as:\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\left(\\mu - \\frac{\\frac{\\overline{y}}{\\sigma^{2}} + \\frac{\\theta}{\\tau^{2}}}{\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}}\\right)^{2}\\left(\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}\\right).\n",
    "\\end{equation*}\n",
    "\n",
    "From this, we can find the posterior mean $\\mu_{post}$ and the posterior variance $\\sigma^{2}_{post}$ to be:\n",
    "\n",
    "\\begin{align*}\n",
    "    & \\mu_{post} = \\frac{\\frac{\\overline{y}}{\\sigma^{2}} + \\frac{\\theta}{\\tau^{2}}}{\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}} \\\\\n",
    "    & \\sigma^{2}_{post} = \\frac{1}{\\frac{n}{\\sigma^{2}} + \\frac{1}{\\tau^{2}}}.\n",
    "\\end{align*}\n",
    "\n",
    "Note that the posterior variance is smaller than either the prior variance or the variance of the sample mean, reflecting the fact that after observing the data, we have more information about $\\mu$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
